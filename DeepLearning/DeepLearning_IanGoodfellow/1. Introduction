A computer is **knowledge based** artificial intelligence which means that it can reason about statements in therese formal languages automatically using logical inference rules.
The ability that AI systems has to acquire their own knowledge by extracting patterns from raw data is known as **machine learning**.
The performance of simple machine learning algorithms depends heavily on the *representation* of the data they are given. However, for many tasks, it is difficult to know what ffeatures should be extracted. One solution is to use machine learning to discover not only *the mapping from representation to output* but also *the representation itself*. This approach is known as **representation learning**.
When designing features or algorithms for learning features, our goal is usually to separate the *factor of variation* that explain the observed data. We use word **factor** refers to separate sources of influence. 
**Deep learning** solves the sophisticated problem in representations learning by introducing representations that are expressed in terms of other, simpler represantations. 
Two ways of measuring the depth of a model are *the number of sequential instructions that must be executed to evaluate the architecture (the depth of the computational graph, 2 times)* or *the depth of the probabilistic modeling graph (2n times)*.
